{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your title for your regression project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Planning\n",
    "\n",
    "### Goals\n",
    "\n",
    "(see background)...your customer is the zillow data science team.  state your goals as if you were delivering this to zillow.  They have asked for something from you (see 'background') and you are basically communicating in a more concise way, and very clearly, the goals as you understand them and as you have taken and acted upon through your research. \n",
    "\n",
    "*Project Overview\n",
    "Background:\n",
    "Zillow wants to improve their Zestimate. The zestimate is estimated value of a home. Zillow theorizes that there is more information to be gained to improve its existing model. Because of that, Zillow wants you to develop a model to predict the error between the Zestimate and the sales price of a home. In predicting the error, you will discover features that will help them improve the Zestimate estimate itself. Your goal of this project is to develop a linear regression model that will best predict the log error of the Zestimate. The error is the difference of the sales price and the Zestimate. The log error is computed by taking the log function of that error. You don't need to worry about the fact that the error is of a logarithmic function. It is a continuous number that represents an error rate.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables\n",
    "\n",
    "What should the zillow team expect to receive from you?  Again, as you were communicating to them, not to your instructors.  \n",
    "\n",
    "*your deliverables:*\n",
    "1. *A report (in the form of a presentation, both verbal and through a slides) that summarizes your findings about the drivers of the Zestimate error. This will come from the analysis you do during the exploration phase of the pipeline. In the report, you will have charts that visually tell the story of what is driving the errors.*\n",
    "\n",
    "2. *A Jupyter notebook, titled 'Regression_Proj_YourName', that contains a clearly labeled section and code clearly documented for each the stages below (project planning, data acquisition, data prep, exploration, and modeling). All of the work will take place in your jupyter notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary & Details\n",
    "\n",
    "**we will be using 2016 data so do NOT use properties_2017 or predictions_2017**  \n",
    "\n",
    "Data dictionary of fields you will use. Why? So that you can refer back and others can refer to the meanings as you are developing your model. This is about gaining knowledge in the domain space so that you will understand when data doesn't look right, be able to more effectively develop hypotheses, and use that domain knowledge to build a more robust model (among other reasons)\n",
    "\n",
    "Define your fields.   You may be able to to some ahead of time, but you may need to return to this section after you have explored the data and understand more about what each field means.  Also, look for a data dictionary either in the database or in the original data source (kaggle.com). \n",
    "\n",
    "You are free to use more fields than is mentioned, but I would recommend not expanding it to too many fields in the beginning as it will add complexity and you want to make sure you get an initial version completed before diving in deeper.  \n",
    "\n",
    "- logerror\n",
    "- bathroomcnt\n",
    "- bedroomcnt\n",
    "- calculatedfinishedsquarefeet\n",
    "- fullbathcnt\n",
    "- garagecarcnt\n",
    "- roomcnt\n",
    "- yearbuilt\n",
    "- taxvaluedollarcnt\n",
    "- taxamount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Thoughts, Ideas, Hypotheses\n",
    "\n",
    "Brainstorming ideas, hypotheses, related to how variables might impact or relate to each other, both within independent variables and between the independent variables and dependent variable, and also related to any ideas for new features you may have while first looking at the existing variables and challenge ahead of you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare your environment\n",
    "\n",
    "import libraries you will use throughout the project.  You may need to add to this as you go.  But try to keep all of your imports in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition\n",
    "\n",
    "#### Acquire via csv\n",
    "\n",
    "- add the path to where your files are located in your env.py file. \n",
    "- ensure that others can read the files based on their local env.py file. \n",
    "\n",
    "- Filter out any from predictions_2016 that do not have a logerror. \n",
    "- Generate a cohesive data set that includes the following fields:\n",
    "\n",
    "- `logerror`\n",
    "- `bathroomcnt`\n",
    "- `bedroomcnt`\n",
    "- `calculatedfinishedsquarefeet`\n",
    "- `fullbathcnt`\n",
    "- `garagecarcnt`\n",
    "- `roomcnt`\n",
    "- `yearbuilt`\n",
    "- `taxvaluedollarcnt`\n",
    "- `taxamount`\n",
    "- (optional) `regionidzip`\n",
    "\n",
    "recommendations for dealing with the large dataset\n",
    "- remove any without a logerror\n",
    "- sample until you have the right query (and then pull entire dataset)\n",
    "- export to local csv \n",
    "- read from local csv (sample, but larger sample than your first) \n",
    "- save sql query to add into to_sql() function in python. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize the data. \n",
    "\n",
    "Why? So you can confirm the data look like what you would expect.\n",
    "\n",
    "- peek at a few rows\n",
    "- data types\n",
    "- summary stats\n",
    "- column names\n",
    "- number of rows and columns\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Goal: leave this section with a dataset that is ready to be analyzed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a variable, `colnames`, that is a list of the column names. \n",
    "Why? You will likely reference this variable later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify the data types of each variable. Why? \n",
    "You need to make sure they are what makes sense for the data and the meaning of the data that lies in that variable. If it does not, make necessary changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify the columns that have missing values and the number of missing values in each column. \n",
    "Why? Missing values are going to cause issues down the line so you will need to handle those appropriately. For each variable with missing values, if it makes sense to replace those missing with a 0, do so. For those where that doesn't make sense, decide if you should drop the entire observations (rows) that contain the missing values, or drop the entire variable (column) that contains the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a list of the independent variable names (aka attributes) and assign it to `x_vars` \n",
    "Why? During exploration, you will likely use this list to refer to the attribute names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clearly identify your dependent (target) variable. \n",
    "- What is the name of the variable? \n",
    "- Is it discrete or continuous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the distribution of the numeric variables\n",
    "- plot a histogram and box plot of each variable. \n",
    "- Why? To see the distribution, skewness, outliers, and unit scales. You will use this information in your decision of whether to normalize, standardize or neither."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS:  Data standardization \n",
    "Create a new data frame that is the min-max normalization of the independent variable in the original data frame (+ the original dependent variable). You will normalize each of the numeric independent variables independently, i.e. using the min and max of each variable, not the min/max of the whole dataframe. Why? Regression is very sensitive to difference in units. It will be almost impossible to extract a meaningful linear regression model with such extreme differences in scale. For more context, see: https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc\n",
    "You will use this dataframe in the future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "Goal is to address each of the questions you posed in your planning & brainstorming through visual or statistical analysis.\n",
    "\n",
    "When you have completed this step, you will have the findings from your analysis that will be used in your final report, the answers to your questions and your customer's questions that will reach the goal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a jointplot for each independent variable (normalized version) with the dependent variable. \n",
    "\n",
    "Be sure you have Pearson's r and p-value annotated on each plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a feature plot using seaborn's PairGrid() of the interaction between each variable (dependent + independent). \n",
    "\n",
    "You may want to use a normalized dataframe (if you did that) or adjust the axis scale (set to logarithmic, e.g) so you can more clearly view the interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a heatmap of the correlation between each variable pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize your conclusions from all of these steps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is the logerror significantly different for homes with 3 bedrooms vs those with 5 or more bedrooms? \n",
    "Run a t-test to test this difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the same for another 2 samples you are interested in comparing (e.g. those with 1 bath vs. x baths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering & Selection\n",
    "\n",
    "#### Are there new features you could create based on existing features that might be helpful? \n",
    "\n",
    "Come up with at least one possible new feature that is a calculation from 2+ existing variables. \n",
    "\n",
    "#### Use statsmodels ordinary least squares to assess the importance of each feature with respect to the target\n",
    "\n",
    "#### Summarize your conclusions and next steps from your analysis in above. \n",
    "What will you try when developing your model? (which features to use/not use/etc)\n",
    "\n",
    "#### Train & Test Model\n",
    "\n",
    "#### Fit, predict (in-sample) & evaluate multiple linear regression models to find the best one.\n",
    "\n",
    "- Make any changes as necessary to improve your model.\n",
    "\n",
    "- Identify the best model after all training\n",
    "\n",
    "- Compare the models by plotting the predictions of each model with actual values (see plot right above section 12 in 'Regression in Python' lesson\n",
    "\n",
    "(the predictions are a 1 x 83 two dimensional matrix, but we want just a single array of predictions. We can use the .ravel method to achieve this.)\n",
    "\n",
    "`pd.DataFrame({'actual': y_train.final_grade,  \n",
    "              'lm1': y_pred_lm1.ravel(),  \n",
    "              'lm2': y_pred_lm2.ravel()})\\  \n",
    "    .melt(id_vars=['actual'], var_name='model', value_name='prediction')\\  \n",
    "    .pipe((sns.relplot, 'data'), x='actual', y='prediction', hue='model')  \n",
    "\n",
    "plt.plot([60, 100], [60, 100], c='black', ls=':')  \n",
    "plt.ylim(60, 100)  \n",
    "plt.xlim(60, 100)  \n",
    "plt.title('Predicted vs Actual Final Grade')`  \n",
    "\n",
    "#### Predict & evaluate on out-of-sample data (test)    \n",
    "\n",
    "- Plot the residuals from your out-of-sample predictions.\n",
    "\n",
    "- Summarize your expectations about how you estimate this model will perform in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
