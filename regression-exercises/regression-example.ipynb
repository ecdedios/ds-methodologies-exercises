{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Churn\n",
    "\n",
    "## Project Planning\n",
    "\n",
    "### Goals\n",
    "\n",
    "We will provide actionable, data-driven recommendations for reducing churn. We will achieve this by analyzing available customer data in order to identify drivers of churn.  We hope to arrive at 1 or more theories related to causal relationships around churn.  \n",
    "\n",
    "### Deliverables\n",
    "\n",
    "1. Notebook detailing my analysis in a way that can be reproduced. \n",
    "2. Report summarizing my findings that can be emailed to all stakeholders\n",
    "3. A presentation to our product and customer relationship leadership team delivering the recommendations and the reasons for those recommendations.  \n",
    "\n",
    "\n",
    "### Data Dictionary & Details\n",
    "\n",
    "- churn:  We define churn as having discontinued all services.  \n",
    "- monthly_charges:  The monthly_charges are the 'current monthly charges' (when the data was collected), or the last charges prior to churn.  \n",
    "- The services listed are those that are present when the data was collected or at the point of churn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pw' from 'env' (/Users/ddfloww/Documents/_datasci/Github/codeup-ds-projects/env.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7d6a3d2d1fb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'pw' from 'env' (/Users/ddfloww/Documents/_datasci/Github/codeup-ds-projects/env.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from env import user, host, pw\n",
    "\n",
    "def get_connection(db, user, host, password):\n",
    "    from sqlalchemy import create_engine\n",
    "    url = f'mysql+pymysql://{user}:{password}@{host}/{db}'\n",
    "    return create_engine(url)\n",
    "\n",
    "conn = get_connection('telco_churn', user, host, pw)\n",
    "\n",
    "df = pd.read_sql('SELECT c.*,p.payment_type, i.internet_service_type, t.contract_type \\\n",
    "                    FROM customers c \\\n",
    "                    JOIN payment_types p ON c.payment_type_id = p.payment_type_id \\\n",
    "                    JOIN internet_service_types i \\\n",
    "                        ON c.internet_service_type_id = i.internet_service_type_id \\\n",
    "                    JOIN contract_types t ON c.contract_type_id = t.contract_type_id;', \n",
    "                 conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = pd.DataFrame(df.columns, columns=['variable_name'])\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_define = ['payment_type', 'internet_service_type', 'contract_type']\n",
    "for var in vars_to_define:\n",
    "    print(var)\n",
    "    print(df[var].value_counts().head())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Thoughts, Ideas, Hypotheses\n",
    "\n",
    "- My experience with telco has demonstrated the best prices come in the beginning and with a contract.  Sign up with a 1 year or 2 year contract and you will get a great deal, along with some free movie channels often.  The free movie channels usually expire after 3-6 months, in my experience.  They remain on the service but the customer begins getting charged for them.  After the sign-on deal is over, the price generally jumps anywhere from 25-50% from the original sign-up prices.  \n",
    "\n",
    "\n",
    "- When customers call to cancel, I have experience an attempt to save the customer by offering another deal along with a contract.  I generally don't take those offers because by the time I reach that point, I am ready to leave. \n",
    "\n",
    "\n",
    "- The addition of fiber internet has added some interesting competition and movement.  Because fiber is in its infancy, the prices have started out higher but continue to drop as more companies are able to offer the option.  I have a hunch fiber might be leading to some migration of customers.  \n",
    "\n",
    "\n",
    "- Senior citizens could impact in a couple of ways I can think of:  \n",
    "\n",
    "    1. They could be less likely to churn, with possible attribution being lack of awareness of where to go or difficulty in adapting to a new service, new remote, etc.\n",
    "    \n",
    "    2. They could be more likely to churn due to pressure from other companies targeting senior citizens and taking advantage of their less experience in technology and the telco services.  \n",
    "    \n",
    "    \n",
    "- I might think that automatic payments, and especially bank transfer, would be less likely to churn, thinking that those with automatic payments are more removed from the cost and any changes in cost, as well as more removed from a negative touchpoint of customer/service provider relationship leaving them to just focus on their experience of the service of tv and/or internet.  \n",
    "\n",
    "\n",
    "- It could be reasonable to think that customers with land-lines are less likely to churn due to the extra challenges of porting numbers and lines.  \n",
    "\n",
    "\n",
    "- I hypothesize that those with the most services and most diversity of services are less likely to churn due to the work required to transfer services.  Especially those with online backup.  \n",
    "\n",
    "\n",
    "- If we control for services and watch the changes in prices over time, is there a difference in the percent increase in price over time for those who churn vs. those who don't?  Or is there a difference in the starting price?  Or a difference in the average price?  Or a difference in the most recent price, controlling for tenure? \n",
    "\n",
    "\n",
    "- It seems likely that month-to-month customers will churn more often, given that they have more opportunities to do so.  If we compare these customers with contract customers and look at annual churn, do those differences still exist?  What are the price differences in month-to-month?  Is there a max point price difference that will drive more customers to contract (if we find overall the contract customers churn less)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "### Environment Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Exploring\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Visualizing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# plt.style.use('classic')\n",
    "\n",
    "# Modeling\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire via SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = get_connection('telco_churn', user, host, pw)\n",
    "\n",
    "df = pd.read_sql('SELECT c.*,p.payment_type, i.internet_service_type, t.contract_type \\\n",
    "                    FROM customers c \\\n",
    "                    JOIN payment_types p ON c.payment_type_id = p.payment_type_id \\\n",
    "                    JOIN internet_service_types i \\\n",
    "                        ON c.internet_service_type_id = i.internet_service_type_id \\\n",
    "                    JOIN contract_types t ON c.contract_type_id = t.contract_type_id;', \n",
    "                 conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_df(df):\n",
    "    print(\"Summary of Data\\n\")\n",
    "    print(\"Rows & Columns:\\n\")\n",
    "    print(df.shape)\n",
    "    print(\"Variables:\\n\")\n",
    "    var_names = df.columns.values\n",
    "    print(var_names)\n",
    "    print(\"\\nColumn Info:\\n\")\n",
    "    print(df.info())\n",
    "    print(\"\\nNumeric Summary Stats:\\n\")\n",
    "    print(df.describe())\n",
    "    print(\"\\nTop 5 Values:\\n\")\n",
    "    for var in var_names:\n",
    "        print(var+\":\")\n",
    "        print(df[var].value_counts().head())\n",
    "        print('\\n')\n",
    "    print(\"\\nMissing Values:\\n\")\n",
    "    null_counts = df.isnull().sum()\n",
    "    if len(null_counts[null_counts > 0]) == 0:\n",
    "        print(\"No missing values\")\n",
    "    else:\n",
    "        print(null_counts[null_counts > 0])\n",
    "    print(\"\\nFirst 5 rows:\\n\")\n",
    "    print(df.head())\n",
    "    print(\"\\nEnd of Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "1. Remove any variables I won't use\n",
    "2. Convert object or category variables to numeric\n",
    "3. Plot the distribution of monthly_charges, total_charges, and tenure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Remove Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['contract_type','contract_type_id'])['customer_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['internet_service_type', 'internet_service_type_id'])['customer_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['payment_type', 'payment_type_id'])['customer_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = ['customer_id', 'internet_service_type', 'contract_type', \n",
    "                  'payment_type', 'phone_service']\n",
    "df_short = df.drop(cols_to_remove, axis=1)\n",
    "df_short.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Convert to Numeric\n",
    "\n",
    "Using the value counts from our initial summary, I will make the following changes to these variables:\n",
    "\n",
    "I can change all of these with the same function: \n",
    "\n",
    "- partner: change to has_partner (1 if 'Yes',0)  \n",
    "\n",
    "- dependents: change to has_dependents (1 if 'Yes',0)  \n",
    "\n",
    "- online_security: change to has_online_security (1 if 'Yes', 0)  \n",
    "\n",
    "- online_backup: change to has_online_backup (1 if 'Yes', 0)  \n",
    "\n",
    "- device_protection: change to has_device_protection (1 if 'Yes', 0)  \n",
    "\n",
    "- tech_support: change to has_tech_support (1 if 'Yes', 0)  \n",
    "\n",
    "- streaming_tv: change to has_streaming_tv (1 if 'Yes', 0)  \n",
    "\n",
    "- streaming_movies: change to has_streaming_movies (1 if 'Yes', 0)  \n",
    "\n",
    "- paperless_billing: change to has_paperless_billing (1 if 'Yes', 0)\n",
    "\n",
    "\n",
    "These I will do separately:\n",
    "\n",
    "- gender:  change to is_female (1 if 'Female', 0)  \n",
    "\n",
    "- multiple_lines: change to phone_service (0 if 'No phone service', 1 if 'No', 2 if 'Yes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_boolean(x):\n",
    "    if x == 'Yes':\n",
    "        new_val = 1\n",
    "    else:\n",
    "        new_val = 0\n",
    "    return new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_bool = ['churn', 'partner', 'dependents', 'online_security', 'online_backup', 'paperless_billing',\n",
    "                'device_protection','tech_support', 'streaming_tv', 'streaming_movies']\n",
    "\n",
    "for col in cols_to_bool:\n",
    "    new_col_name = 'has_'+col\n",
    "    df_short[new_col_name] = df_short[col].apply(lambda x: change_to_boolean(x))\n",
    "    df_short = df_short.drop([col], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_female(x):\n",
    "    if x == 'Female':\n",
    "        new_val = 1\n",
    "    else:\n",
    "        new_val = 0\n",
    "    return new_val\n",
    "\n",
    "def phone_service(x):\n",
    "    if x == 'Yes':\n",
    "        new_val = 2\n",
    "    elif x == 'No':\n",
    "        new_val = 1\n",
    "    else:\n",
    "        new_val = 0\n",
    "    return new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short['is_female'] = df_short['gender'].apply(lambda x: is_female(x))\n",
    "df_short['phone_service_id'] = df_short['multiple_lines'].apply(lambda x: phone_service(x))\n",
    "\n",
    "df_short = df_short.drop(['gender', 'multiple_lines'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that total_charges is the only remaining non-numeric variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short.sort_values(['total_charges'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short['total_charges'] = df_short['total_charges'].replace(r'\\s+', np.nan, regex=True)\n",
    "df_short.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to numeric\n",
    "df_short = df_short.fillna(0)\n",
    "df_short.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Plot the distribution\n",
    "\n",
    "##### monthly_charges, total_charges, and tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "for i, col in enumerate(['monthly_charges', 'total_charges', 'tenure']):\n",
    "    plot_number = i + 1\n",
    "    series = df_short[col]\n",
    "    plt.subplot(1,3,plot_number)\n",
    "    plt.title(col)\n",
    "    series.hist(bins=20, density=True, cumulative=False, log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_.drop(['churn']\n",
    "y = df[['churn']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.7, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('white'):\n",
    "    j = sns.jointplot(\"monthly_charges\", \"total_charges\", data=train, kind='reg', height=5);\n",
    "    j.annotate(stats.pearsonr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "### Background: \n",
    "\n",
    "\n",
    "Zillow wants to improve their Zestimate.  The zestimate is estimated value of a home.  Zillow theorizes that there is more information to be gained to improve its existing model.  Because of that, Zillow wants you to develop a model to predict the error between the Zestimate and the sales price of a home.  In predicting the error, you will discover features that will help them improve the Zestimate estimate itself.  Your goal of this project is to develop a linear regression model that will best predict the log error of the Zestimate.  The error is the difference of the sales price and the Zestimate.  The log error is computed by taking the log function of that error.  You don't need to worry about the fact that the error is of a logarithmic function.  It is a continuous number that represents an error rate. \n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "1. Sample the data.  Why?  So you can confirm the data look like what you would expect.\n",
    "2. Create a variable, `colnames`, that is a list of the column names.  Why?  You will likely reference this variable later. \n",
    "3. Identify the data types of each variable.  Why? You need to make sure they are what makes sense for the data and the meaning of the data that lies in that variable.  If it does not, make necessary changes.  \n",
    "4. Compute the summary statistics for the variables.  Why?  The get a glimpse into outliers, skewness, spread, central tendency.  \n",
    "5. Identify the columns that have missing values and the number of missing values in each column. Why? Missing values are going to cause issues down the line so you will need to handle those appropriately.  For each variable with missing values, if it makes sense to replace those missing with a 0, do so.  For those where that doesn't make sense, decide if you should drop the entire observations (rows) that contain the missing values, or drop the entire variable (column) that contains the missing values. \n",
    "6. Create a list of the independent variable names (aka attributes) and assign it to the variable `attributes`. Why? During exploration, you will likely use this list to refer to the attribute names. \n",
    "7. Clearly identify your dependent (target) variable.  What is the name of the variable? Is it discrete or continuous?\n",
    "8. Plot a histogram and box plot of each variable.  Why?  To see the distribution, skewness, outliers, and unit scales.  You will use this information in your decision of whether to normalize, standardize or neither. \n",
    "9. Bonus: Create a new data frame that is the min-max normalization of the independent variable in the original data frame (+ the original dependent variable).  You will normalize each of the independent variables independently, i.e. using the min and max of each variable, not the min/max of the whole dataframe. Why?  Regression is very sensitive to difference in units.  It will be almost impossible to extract a meaningful linear regression model with such extreme differences in scale.  For more context, see: https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc\n",
    "\n",
    "\n",
    "## Data Exploration\n",
    "\n",
    "1. Split data into training and test datasets\n",
    "2. Address each of the questions you posed in your planning & brainstorming through visual or statistical analysis.  \n",
    "3. Create a jointplot for each independent variable (normalized version) with the dependent variable.  Use your for loop created in the exercises to run through the plotting of each independent variable.  Be sure you have Pearson's r and p-value annotated on each plot.  \n",
    "4. Create a feature plot using seaborn's PairGrid() of the interaction between each variable (dependent + independent).  You will want to use the normalized dataframe so you can more clearly view the interactions.  \n",
    "5. Create a heatmap of the correlation between each variable pair.  \n",
    "6. Summarize your conclusions from these steps. \n",
    "7. Is the logerror significantly different for homes with 3 bedrooms vs those with 5 or more bedrooms?  Run a t-test to test this difference.  \n",
    "8. Do the same for another 2 samples you are interested in comparing (e.g. those with 1 bath vs. x baths)\n",
    "\n",
    "## Data Modeling\n",
    "\n",
    "### Feature Engineering & Selection\n",
    "\n",
    "1. Are there new features you could create based on existing features that might be helpful?  Come up with at least one possible new feature that is a calculation from 2+ existing variables.  Add that feature and update the normalized dataframe with the min-max normalization of that feature. \n",
    "2. Use statsmodels ordinary least squares to assess the importance of each feature with respect to the target (using the normalized dataframe)\n",
    "3. Summarize your conclusions and next steps from your analysis in step 2.  What will you try when developing your model?  (which features to use/not use/etc)\n",
    "\n",
    "#### Train & Test Model\n",
    "\n",
    "1. Fit, predict (in-sample) & evaluate multiple linear regression models to find the best one.\n",
    "2. Make any changes as necessary to improve your model.\n",
    "3. Identify the best model after all training and predict & evaluate on out-of-sample data.  \n",
    "4. Plot the residuals from your out-of-sample predictions.  \n",
    "5. Summarize your expectations about how you estimate this model will perform in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in Python\n",
    "\n",
    "\n",
    "## Goals\n",
    "\n",
    "### Learning Agenda\n",
    "\n",
    "Use Python (Pandas, Matplotlib, Scipy, Scikit-Learn) to perform regression\n",
    "\n",
    "### Example Challenge\n",
    "\n",
    "1. Predict students' final grades using the first exam of the semester\n",
    "2. Predict students' final grades using the first three exams of the semester\n",
    "\n",
    "## The Plan\n",
    "\n",
    "1. Prepare the Environment\n",
    "1. Pandas:  read a local csv\n",
    "2. Pandas:  sample and summarize\n",
    "3. Pandas:  ensure no null values\n",
    "4. SKlearn: split into test/train\n",
    "6. Matplotlib & Seaborn: Explore\n",
    "7. Scipy: Pearson's correlation\n",
    "8. Statsmodels: Feature Selection\n",
    "9. Scikit-Learn: Fit Linear Regression Models, In-Sample Predictions\n",
    "10. Scikit-Learn: In-Sample Evaluations\n",
    "11. Scikit-Learn: Make any changes needed & repeat 9-10 as needed\n",
    "12. Scikit-Learn: Out-of-sample predictions using best model\n",
    "13. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Wrangling\n",
    "import pandas as pd\n",
    "\n",
    "# Exploring\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Visualizing\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('classic')\n",
    "\n",
    "# Modeling\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pandas:  read a local csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "df = pd.read_csv(path + \"student_grades.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pandas:  sample and summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Exercise\n",
    "Extract information from the results of describe in order to compute IQR and range\n",
    "> ```python\n",
    "> IQR = \n",
    "> Range = \n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pandas:  ensure no null values\n",
    "\n",
    "The above can tell you something about missing values when you compare the *entries* with each fields *non-null* values.  You can also do this for a more direct compuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "\n",
    "# or  print(df.columns[df.isnull().any()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Distribution, Skewness, Normalization & Standardization\n",
    "\n",
    "Plot histograms &/or boxplots to see the distribution, skewness, outliers, and unit scales.  You will use this information in your decision of whether to normalize, standardize or neither. \n",
    "\n",
    "#### Histogram\n",
    "\n",
    "There are 2 ways we can go about plotting histograms for multiple variables. \n",
    "\n",
    "1. melt the data frame into a long data set and use seaborn's 'FacetGrid' to plot each histogram quickly.\n",
    "2. use matplotlib with subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Melt data and use FacetGrid to plot histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create a melted version of train for visualization purposes\n",
    "df_melted = pd.melt(df, id_vars=['student_id'], value_vars=['exam1','exam2','exam3','final_grade'], \n",
    "                    var_name='grade_type', value_name='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted['grade_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted['score'].value_counts(bins=10, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "grid = sns.FacetGrid(df_melted, col=\"grade_type\", margin_titles=True)\n",
    "grid.map(plt.hist, \"score\", bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(df_melted, col=\"grade_type\", margin_titles=True)\n",
    "grid.map(plt.hist, \"score\", bins=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using matplotlib with subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "for i, col in enumerate(['exam1', 'exam2', 'exam3', 'final_grade']):  \n",
    "    plot_number = i + 1 # i starts at 0, but plot nos should start at 1\n",
    "    series = df[col]  \n",
    "    plt.subplot(2, 2, plot_number)\n",
    "    plt.title(col)\n",
    "    series.hist(bins=20, density=False, cumulative=False, log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the above loop step by step. \n",
    "\n",
    "> 1. enumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_col_list = list(enumerate(['exam1', 'exam2', 'exam3', 'final_grade']))\n",
    "print(for_list)\n",
    "ex1_ix_col_tuple = for_list[0]\n",
    "print(ex1_ix_col_tuple) \n",
    "ex1_index = ex1_ix_col_tuple[0]\n",
    "print(ex1_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. Assign the first index, 0, to i\n",
    "> 3. Assign the first column name, 'exam1', to col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = ex1_ix_col_tuple[0]\n",
    "print(\"The first item of the tuple is the index.  The first index = %.0f\" % i)\n",
    "col = ex1_ix_col_tuple[1]\n",
    "print(\"The second item of the tuple is the column name.  The first column name = %s\" % col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. Using index = 0 and column of 'exam1', assign the variable plot_number to the first plot (1), and create a series of all the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_number = i + 1 # i starts at 0, but plot nos should start at 1\n",
    "print(\"plot_number: %.0f\" % plot_number)\n",
    "series = df[col]\n",
    "print(\"Number of grades in the series: %.0f\" % series.shape[0])\n",
    "print(series.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5. Create the subplot object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, plot_number) # plot number = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 6. Add the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, plot_number) # plot number = 1\n",
    "plt.title(col) # plot title = 'exam1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 7. Add the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, plot_number) # plot number = 1\n",
    "plt.title(col) # plot title = 'exam1'\n",
    "series.hist(bins=20, density=False, cumulative=False, log=False) # plot the histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Plot\n",
    "\n",
    "seaborn will default to plotting *all* the numeric variables if we don't specify specific x and y values. This is the behavior we want, except we don't want to plot the `student_id` column.\n",
    "\n",
    "We'll use the `.drop` method to remove the `student_id` column from the data frame. Recall that `.drop` will produce a new data frame without the specified column(s), but will not modify the original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['student_id']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(data=df.drop(columns=['student_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. SKlearn: split into test/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_train_split\n",
    "X = df.drop(['final_grade','student_id'], axis=1)\n",
    "print(X.head())\n",
    "\n",
    "y = df[['final_grade']]\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "print(\"Train columns:  %s\" % list(train.columns))\n",
    "print(\"Train dimensions (rows, columns):\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "print(\"Test columns:  %s\" % list(test.columns))\n",
    "print(\"Test dimensions (rows, columns):\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show some python code that validates that the splits are what we would expect. That is, we ensure that:\n",
    "\n",
    "- the number of rows in both the x and y training data set are equal\n",
    "- the number of rows in both the x and y testing data set are equal\n",
    "- the number of columns in the training and test data sets are the same\n",
    "- the training data set is 80% of the original data, the test set is 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train.shape[0] == y_train.shape[0]:\n",
    "    print(\"X & y train rows ARE equal\")\n",
    "else:\n",
    "    print(\"X & y train rows ARE NOT equal\")\n",
    "\n",
    "\n",
    "if X_test.shape[0] == y_test.shape[0]:\n",
    "    print(\"X & y test rows ARE equal\")\n",
    "else:\n",
    "    print(\"X & y test rows ARE NOT equal\")\n",
    "\n",
    "if train.shape[1] == test.shape[1]:\n",
    "    print(\"Number of columns in train & test ARE equal\")\n",
    "else:\n",
    "    print(\"Number of columns in train & test ARE NOT equal\")\n",
    " \n",
    "train_split = train.shape[0] / (train.shape[0] + test.shape[0])\n",
    "test_split = test.shape[0] / (train.shape[0] + test.shape[0])\n",
    "\n",
    "print(\"Train Split: %.2f\" % train_split)\n",
    "print(\"Test Split: %.2f\" % test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Matplotlib & Seaborn: Explore\n",
    "\n",
    "- Graphs are many and are quickly made\n",
    "- Axis and labels are cleaned up later\n",
    "- Color and size are used for information, not asthetics\n",
    "- Graph types include box plts, heatmaps, histograms, density plots, feature or correlation plots  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Jointplot: Scatterplot with Density Plots\n",
    "2. PairGrid: Scatterplots with Histograms\n",
    "3. Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplot + Density Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('white'):\n",
    "    j = sns.jointplot(\"exam1\", \"final_grade\", data=train, kind='reg', height=5);\n",
    "    j.annotate(stats.pearsonr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1_Exercise\n",
    "\n",
    "1. Write a for loop to plot each of the 3 features with final_grade using a jointplot.\n",
    "2. What can you say about each relationship (using pearson's r and the p-value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram + Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is roughly equivalent to sns.jointplot, but we see here that we have the\n",
    "# flexibility to customize the type of the plots in each position.\n",
    "\n",
    "g = sns.PairGrid(train)\n",
    "g.map_diag(plt.hist)\n",
    "g.map_offdiag(plt.scatter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.heatmap(train.corr(), cmap='Blues', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2_Exercise\n",
    "\n",
    "1. Plot a boxplot and heatmap side by side using subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Scipy: Pearson's Correlation\n",
    "\n",
    "#### stats.pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(X_train[['exam1']], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List Comprehension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[column, pearsonr(train[column], train.final_grade)] for column in X_train] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = X_train.columns\n",
    "r_and_p_values = [pearsonr(X_train[col], y_train.final_grade) for col in column_names]\n",
    "\n",
    "exam_final_corr_dict = dict(zip(column_names, r_and_p_values))\n",
    "exam_final_corr_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Statsmodels: Feature Selection\n",
    "\n",
    "#### Feature selection using statsmodels.OLS (Ordinary Least Squares)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = sm.OLS(y_train, X_train)\n",
    "fit = ols_model.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "Looking at the p-values of the 3 exams, I am going to remove exam2 from the features as we are not seeing any significance of that with the final grade, our target variable.\n",
    "\n",
    "I will now use sklearn to create a model using exam 1, exam3 and exam 1 & 3 to compare results.\n",
    "\n",
    "(well, I will model exam 1 (lm1) and exam 1 & 3 (lm2), and you will test exam 3 alone (lm3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Scikit-Learn: Fit Linear Regression Models, In-Sample Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, we'll use scikit-learn like this:\n",
    "\n",
    "1. Import the class(es) that we intend on using (we've already done this)\n",
    "1. Split the data into training and test (we've already done this)\n",
    "1. Create an instance of the class(es) to use\n",
    "1. Fit the model(s) with the training data\n",
    "1. Use the model(s) to make predictions\n",
    "1. Evaluate the performance of the model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create two separate models: one that is based solely on exam 1, and one that is based on exam 1 and exam 2.\n",
    "\n",
    "#### Linear Model 1\n",
    "\n",
    "##### Create the linear regression object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression objects\n",
    "lm1 = LinearRegression()\n",
    "print(lm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit/Train the model\n",
    "\n",
    "This is where we feed the training data into the models, and model parameters are \"learned\". \n",
    "\n",
    "Let's take a look at the linear model and the parameters (y_intercept & coefficients) that the model calulated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm1.fit(X_train[['exam1']], y_train)\n",
    "print(lm1)\n",
    "\n",
    "lm1_y_intercept = lm1.intercept_\n",
    "print(lm1_y_intercept)\n",
    "\n",
    "lm1_coefficients = lm1.coef_\n",
    "print(lm1_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the intercept and the coefficients, we can write the regression function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Univariate - final_exam = b + m * exam1')\n",
    "print('    y-intercept (b): %.2f' % lm1_y_intercept)\n",
    "print('    coefficient (m): %.2f' % lm1_coefficients[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!tip \"sklearn conventions\"\n",
    "    Sklearn uses an underscore suffic to indicate properties which are learned from the data (as opposed to being inherit to the model itself.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In-Sample Prediction\n",
    "\n",
    "Now that we have a model, we can use the model to make predictions. We'll start by using our model to predict based on the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lm1 = lm1.predict(X_train[['exam1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/In-Sample Evaluations\n",
    "using sklearn.mean_squared_error and sklearn.r2_score.\n",
    "\n",
    "Now let's take a look at a couple performance metrics for our models: the mean squared error and r-squared values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_lm1 = mean_squared_error(y_train, y_pred_lm1)\n",
    "print(\"lm1\\n  mse: {:.3}\".format(mse_lm1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_lm1 = r2_score(y_train, y_pred_lm1)\n",
    "\n",
    "print('  {:.2%} of the variance in the student''s final grade can be explained by the grade on the first exam.'.format(r2_lm1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear model 2: lm2\n",
    "\n",
    "#### Create the linear regression object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = LinearRegression()\n",
    "print(lm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit/Train the model\n",
    "\n",
    "This is where we feed the training data into the models, and model parameters are \"learned\". \n",
    "\n",
    "Let's take a look at the linear model and the parameters (y_intercept & coefficients) that the model calulated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.fit(X_train[['exam1', 'exam3']], y_train)\n",
    "print(lm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the parameters (y_intercept & coefficients) that the models calulated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2_y_intercept = lm2.intercept_\n",
    "print(lm2_y_intercept)\n",
    "\n",
    "lm2_coefficients = lm2.coef_\n",
    "print(lm2_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the intercept and the coefficients, we can write the regression function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Multivariate - final_exam = b + m1 * exam1 + m2 * exam3')\n",
    "print('    y-intercept  (b): %.2f' % lm2_y_intercept)\n",
    "print('    coefficient (m1): %.2f' % lm2_coefficients[0][0])\n",
    "print('    coefficient (m2): %.2f' % lm2_coefficients[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In-Sample Prediction\n",
    "\n",
    "Now that we have a model, we can use the model to make predictions. We'll start by using our model to predict based on the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lm2 = lm2.predict(X_train.drop(columns=['exam2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/In-Sample Evaluations\n",
    "using sklearn.mean_squared_error and sklearn.r2_score.\n",
    "\n",
    "Now let's take a look at a couple performance metrics for our models: the mean squared error and r-squared values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_lm2 = mean_squared_error(y_train, y_pred_lm2)\n",
    "print(\"lm2\\n  mse: {:.3}\".format(mse_lm2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_lm2 = r2_score(y_train, y_pred_lm2)\n",
    "print('  {:.2%} of the variance in the student''s final grade can be explained by the grades on exam 1 and 3.'.format(r2_lm2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare the 2 models\n",
    "We can visualize the predictions by plotting the predictions vs the actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the predictions are a 1 x 83 two dimensional matrix, but we want just\n",
    "# a single array of predictions. We can use the .ravel method to achieve\n",
    "# this.\n",
    "\n",
    "pd.DataFrame({'actual': y_train.final_grade,\n",
    "              'lm1': y_pred_lm1.ravel(),\n",
    "              'lm2': y_pred_lm2.ravel()})\\\n",
    "    .melt(id_vars=['actual'], var_name='model', value_name='prediction')\\\n",
    "    .pipe((sns.relplot, 'data'), x='actual', y='prediction', hue='model')\n",
    "\n",
    "plt.plot([60, 100], [60, 100], c='black', ls=':')\n",
    "plt.ylim(60, 100)\n",
    "plt.xlim(60, 100)\n",
    "plt.title('Predicted vs Actual Final Grade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the line down the middle represents \"perfect\" prediction. The further away from this line the are, the farther off the prediction was."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Scikit-Learn: Make any changes needed & repeat 9-10 as needed\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "1. What are your take-aways from the evaluation results above? \n",
    "2. Fit, predict, and evaluate one more model, lm3, using exam 3 only.\n",
    "3. How does the result of the model compare to the other two?  Which should you use, based on these results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Scikit-Learn: Out-of-sample predictions using best model\n",
    "\n",
    "#### Predict final grades using testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the testing set\n",
    "y_pred_lm2 = lm2.predict(X_test[['exam1', 'exam3']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred_lm2)\n",
    "\n",
    "print(\"Mean squared error: %.2f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test, y_pred_lm2)\n",
    "\n",
    "print('{:.2%} of the variance in the student''s final grade can be explained by the grades on the first 3 exams.'\n",
    "      .format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_pred_lm2, y_pred_lm2 - y_test, c='g', s=40)\n",
    "plt.hlines(y=0, xmin=50, xmax=100)\n",
    "plt.title(\"Residual plot\")\n",
    "plt.ylabel('Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- [Standardization vs Normalization](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc)\n",
    "- [Visualization with Seaborn](https://jakevdp.github.io/PythonDataScienceHandbook/04.14-visualization-with-seaborn.html)\n",
    "- [Multiple Regression with `statsmodels`](https://nbviewer.jupyter.org/urls/s3.amazonaws.com/datarobotblog/notebooks/multiple_regression_in_python.ipynb    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises:\n",
    "\n",
    "1. Using the example data above:\n",
    "\n",
    "    1. Extract information from the results of `.describe` and compute IQR and\n",
    "       range of each exam and the final grade\n",
    "    1. Write a for loop to plot each of the 3 features (exam 1, 2, & 3) with the\n",
    "       final grade using a joint plot.  What can you say about each relationship\n",
    "       (using pearson's r and the p-value)\n",
    "    1. Plot the boxplot & heatmap side by side using 'subplot'\n",
    "    1. What are your take-aways from the evaluation results of `lm1` and `lm2`?\n",
    "    1. Fit, predict, and evaluate one more model, `lm3`, using exam 3 only.\n",
    "    1. How does the result of the model compare to the other two?  Which should\n",
    "       you use, based on these results?\n",
    "    1. Predict final grades of the out-of-sample data (test dataframe) and\n",
    "       evaluate results.\n",
    "    1. Which model performs the best, based on the 2 out-of-sample evaluation\n",
    "       results (the example one, `lm1`, and the exercise one, `lm3`, from the\n",
    "       above exercise).\n",
    "\n",
    "1. Using `telco_churn` database, predict total charges\n",
    "\n",
    "    1. Using the `telco_churn` database, extract a table for each customer *with\n",
    "       a 2-year contract* and include the following information customer id,\n",
    "       tenure, monthly charges, and total charges.\n",
    "    1. Export the table to a csv\n",
    "    1. Create a jupyter notebook titled `regression`\n",
    "    1. Prepare the Environment (do not copy and paste...manually type so that\n",
    "       you remember what you need and why you are importing the things that you\n",
    "       are. You will do this often!)\n",
    "    1. Read the information from the `customers` table into a dataframe.\n",
    "    1. How many rows are in your dataframe?\n",
    "    1. What is the data type of the field containing the customer ids?\n",
    "    1. Print to the notebook the last 10 rows of your dataframe.\n",
    "    1. What is the inner quartile range of tenure? monthly charges? total\n",
    "       charges?\n",
    "    1. How many missing values are in each variable?\n",
    "    1. Fill any missing values with 0.\n",
    "    1. Show the ditribution of monthly charges through a histogram\n",
    "    1. Create a new dataframe that contains tenure, monthly charges, and total\n",
    "       charges, but not the customer id.\n",
    "    1. Create a box plot of each variable in your new dataframe. This should be\n",
    "       a single chart with 3 categorical plots, 1 each for tenure, monthly\n",
    "       charges, total charges.\n",
    "    1. Split your data into a test and train dataset. Set the `random_state` to\n",
    "       `123`.  You should end up with 4 dataframes:  `X_train`, `X_test`,\n",
    "       `y_train`, `y_test`.\n",
    "\n",
    "1. Using the in-sample data (`X_train` and `y_train`)\n",
    "\n",
    "    1. Create a scatterplot for each combination of variables.\n",
    "    1. Create a heatmap containing the 3 variables.\n",
    "    1. Compute pearson's correlation coefficient and print it's value in the\n",
    "       sentence \"Pearson's R is `____` with a significance p-value of `____`\"\n",
    "    1. Train (aka *fit*) a linear regression model, modeling total charges as a\n",
    "       linear function of tenure.\n",
    "    1. What is the y-intercept of the regression line?  Translate the intercept\n",
    "       in plain english, i.e. what is means in the context of the data.\n",
    "    1. What the slope of the regression line?  Translate the slope in plain\n",
    "       english, i.e. what is means in the context of the data.\n",
    "    1. Write the linear function in the form of $y = mx + b$ using the\n",
    "       parameters that were estimated from the algorithm and the variable names\n",
    "       for y and x specific to your data.\n",
    "    1. Test your model on the training sample. That is, use your newly fit model\n",
    "       to predict the total charges using tenure.\n",
    "    1. Evaluate the model's performance using r-squared, mean squared error, and\n",
    "       median absolute error.\n",
    "\n",
    "1. Using the out-of-sample data (`X_test`, `y_test`)\n",
    "\n",
    "    1. Test your model on the test sample.\n",
    "    1. Evaluate your model's performance on test sample.  How do each of the\n",
    "       metrics compare to that of the training sample? How can you explain the\n",
    "       reason for the difference?\n",
    "    1. Calculate the p-value of the regressor for the train and test sample.\n",
    "       Hint: f_regression\n",
    "    1. Create a scatter plot of the predicted values versus the actual values in\n",
    "       the test sample.\n",
    "    1. Create a scatter plot of the actual values of x and y in the test sample\n",
    "       with the regression line layered over.\n",
    "    1. Create a scatter plot of the residuals (x axis: predicted values of final\n",
    "       grade, y axis: the residual values (remember definition of residual from\n",
    "       previous lesson).  Layer over it a horizonal line that represents no\n",
    "       residual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
